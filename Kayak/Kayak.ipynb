{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Plan your trip with Kayak \n",
    "\n",
    "## Company's description üìá\n",
    "\n",
    "<a href=\"https://www.kayak.com\" target=\"_blank\">Kayak</a> is a travel search engine that helps user plan their next trip at the best price.\n",
    "\n",
    "The company was founded in 2004 by Steve Hafner & Paul M. English. After a few rounds of fundraising, Kayak was acquired by <a href=\"https://www.bookingholdings.com/\" target=\"_blank\">Booking Holdings</a> which now holds: \n",
    "\n",
    "* <a href=\"https://booking.com/\" target=\"_blank\">Booking.com</a>\n",
    "* <a href=\"https://kayak.com/\" target=\"_blank\">Kayak</a>\n",
    "* <a href=\"https://www.priceline.com/\" target=\"_blank\">Priceline</a>\n",
    "* <a href=\"https://www.agoda.com/\" target=\"_blank\">Agoda</a>\n",
    "* <a href=\"https://Rentalcars.com/\" target=\"_blank\">RentalCars</a>\n",
    "* <a href=\"https://www.opentable.com/\" target=\"_blank\">OpenTable</a>\n",
    "\n",
    "With over \\$300 million revenue a year, Kayak operates in almost all countries and all languages to help their users book travels accros the globe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project üöß\n",
    "\n",
    "The marketing team needs help on a new project. After doing some user research, the team discovered that **70% of their users who are planning a trip would like to have more information about the destination they are going to**. \n",
    "\n",
    "In addition, user research shows that **people tend to be defiant about the information they are reading if they don't know the brand** which produced the content. \n",
    "\n",
    "Therefore, Kayak Marketing Team would like to create an application that will recommend where people should plan their next holidays. The application should be based on real data about:\n",
    "\n",
    "* Weather \n",
    "* Hotels in the area \n",
    "\n",
    "The application should then be able to recommend the best destinations and hotels based on the above variables at any given time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals üéØ\n",
    "\n",
    "As the project has just started, your team doesn't have any data that can be used to create this application. Therefore, your job will be to: \n",
    "\n",
    "* Scrape data from destinations \n",
    "* Get weather data from each destination \n",
    "* Get hotels' info about each destination\n",
    "* Store all the information above in a data lake\n",
    "* Extract, transform and load cleaned data from your datalake to a data warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scope of this project üñºÔ∏è\n",
    "\n",
    "Marketing team wants to focus first on the best cities to travel to in France. According <a href=\"https://one-week-in.com/35-cities-to-visit-in-france/\" target=\"_blank\">One Week In.com</a> here are the top-35 cities to visit in France: \n",
    "\n",
    "```python \n",
    "[\"Mont Saint Michel\",\n",
    "\"St Malo\",\n",
    "\"Bayeux\",\n",
    "\"Le Havre\",\n",
    "\"Rouen\",\n",
    "\"Paris\",\n",
    "\"Amiens\",\n",
    "\"Lille\",\n",
    "\"Strasbourg\",\n",
    "\"Chateau du Haut Koenigsbourg\",\n",
    "\"Colmar\",\n",
    "\"Eguisheim\",\n",
    "\"Besancon\",\n",
    "\"Dijon\",\n",
    "\"Annecy\",\n",
    "\"Grenoble\",\n",
    "\"Lyon\",\n",
    "\"Gorges du Verdon\",\n",
    "\"Bormes les Mimosas\",\n",
    "\"Cassis\",\n",
    "\"Marseille\",\n",
    "\"Aix en Provence\",\n",
    "\"Avignon\",\n",
    "\"Uzes\",\n",
    "\"Nimes\",\n",
    "\"Aigues Mortes\",\n",
    "\"Saintes Maries de la mer\",\n",
    "\"Collioure\",\n",
    "\"Carcassonne\",\n",
    "\"Ariege\",\n",
    "\"Toulouse\",\n",
    "\"Montauban\",\n",
    "\"Biarritz\",\n",
    "\"Bayonne\",\n",
    "\"La Rochelle\"]\n",
    "```\n",
    "\n",
    "Your team should focus **only on the above cities for your project**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers ü¶Æ\n",
    "\n",
    "To help you achieve this project, here are a few tips that should help you\n",
    "\n",
    "### Get weather data with an API \n",
    "\n",
    "*   Use https://nominatim.org/ to get the gps coordinates of all the cities (no subscription required) Documentation : https://nominatim.org/release-docs/develop/api/Search/\n",
    "\n",
    "*   Use https://openweathermap.org/appid (you have to subscribe to get a free apikey) and https://openweathermap.org/api/one-call-api to get some information about the weather for the 35 cities and put it in a DataFrame\n",
    "\n",
    "*   Determine the list of cities where the weather will be the nicest within the next 7 days For example, you can use the values of daily.pop and daily.rain to compute the expected volume of rain within the next 7 days... But it's only an example, actually you can have different opinions on a what a nice weather would be like üòé Maybe the most important criterion for you is the temperature or humidity, so feel free to change the rules !\n",
    "\n",
    "*   Save all the results in a `.csv` file, you will use it later üòâ You can save all the informations that seem important to you ! Don't forget to save the name of the cities, and also to create a column containing a unique identifier (id) of each city (this is important for what's next in the project)\n",
    "\n",
    "*   Use plotly to display the best destinations on a map\n",
    "\n",
    "### Scrape Booking.com \n",
    "\n",
    "Since BookingHoldings doesn't have aggregated databases, it will be much faster to scrape data directly from booking.com \n",
    "\n",
    "You can scrap as many information asyou want, but we suggest that you get at least:\n",
    "\n",
    "*   hotel name,\n",
    "*   Url to its booking.com page,\n",
    "*   Its coordinates: latitude and longitude\n",
    "*   Score given by the website users\n",
    "*   Text description of the hotel\n",
    "\n",
    "\n",
    "### Create your data lake using S3 \n",
    "\n",
    "Once you managed to build your dataset, you should store into S3 as a csv file. \n",
    "\n",
    "### ETL \n",
    "\n",
    "Once you uploaded your data onto S3, it will be better for the next data analysis team to extract clean data directly from a Data Warehouse. Therefore, create a SQL Database using AWS RDS, extract your data from S3 and store it in your newly created DB. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable üì¨\n",
    "\n",
    "To complete this project, your team should deliver:\n",
    "\n",
    "* A `.csv` file in an S3 bucket containing enriched information about weather and hotels for each french city\n",
    "\n",
    "* A SQL Database where we should be able to get the same cleaned data from S3 \n",
    "\n",
    "* Two maps where you should have a Top-5 destinations and a Top-20 hotels in the area. You can use plotly or any other library to do so. It should look something like this: \n",
    "\n",
    "![Map](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/Kayak_best_destination_project.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import librairies\n",
    "\n",
    "#API AND SCRAPPING\n",
    "import requests\n",
    "import json\n",
    "import os \n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "#DATA MANIPULATION\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "#VISUALISATION\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.io as pio\n",
    "\n",
    "#STORAGE\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "import sqlalchemy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des villes √† visiter*\n",
    "# correction des villes avec - et lieux touristiques >villes\n",
    "cities_list= [\n",
    "\"Le Mont-Saint-Michel\",\n",
    "\"Saint-Malo\",\n",
    "\"Bayeux\",\n",
    "\"Le Havre\",\n",
    "\"Rouen\",\n",
    "\"Paris\",\n",
    "\"Amiens\",\n",
    "\"Lille\",\n",
    "\"Strasbourg\",\n",
    "\"Orschwiller\", #Chateau du Haut Koenigsbourg\"\n",
    "\"Colmar\",\n",
    "\"Eguisheim\",\n",
    "\"Besancon\",\n",
    "\"Dijon\",\n",
    "\"Annecy\",\n",
    "\"Grenoble\",\n",
    "\"Lyon\",\n",
    "\"La Palud-sur-Verdon\", #Gorge du Verdon\n",
    "\"Bormes-les-Mimosas\",\n",
    "\"Cassis\",\n",
    "\"Marseille\",\n",
    "\"Aix-en-Provence\",\n",
    "\"Avignon\",\n",
    "\"Uzes\",\n",
    "\"Nimes\",\n",
    "\"Aigues-Mortes\",\n",
    "\"Saintes-Maries-de-la-mer\",\n",
    "\"Collioure\",\n",
    "\"Carcassonne\",\n",
    "\"Ariege\",\n",
    "\"Toulouse\",\n",
    "\"Montauban\",\n",
    "\"Biarritz\",\n",
    "\"Bayonne\",\n",
    "\"La Rochelle\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'nom': 'Uz√®s',\n",
       "  'centre': {'type': 'Point', 'coordinates': [4.416, 44.0289]},\n",
       "  'code': '30334',\n",
       "  '_score': 0.739517248275684}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R√©cup√©ration des coordonn√©es g√©ographiques des villes\n",
    "\n",
    "# Envoyer une requ√™te GET √† l'API geo.api.gouv.fr pour chaque ville\n",
    "url = \"https://geo.api.gouv.fr/communes?nom=Uzes&fields=nom,centre,boost=population&limit=1\"\n",
    "response = requests.get(url)\n",
    "response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitudes: [48.6245, 48.6463, 49.2772, 49.4958, 49.4412, 43.7968, 49.8987, 48.5703, 48.5691, 48.2468, 48.1115, 48.0377, 47.2602, 47.3319, 45.9024, 45.1842, 45.758, 43.801, 43.1575, 43.2185, 43.2119, 43.536, 43.9416, 44.0289, 43.8322, 43.5482, 43.4958, 42.5087, 43.2078, 42.9396, 43.6007, 44.0217, 43.471, 43.4844, 46.1621]\n",
      "Longitudes: [-1.5278, -2.0066, -0.7016, 0.1312, 1.0912, 1.8296, 2.2847, -1.8538, 7.7621, 7.3627, 7.3924, 7.2966, 6.0123, 5.0322, 6.1264, 5.7155, 4.8351, 6.323, 6.3615, 5.5503, 2.5438, 5.3879, 4.8333, 4.416, 4.3429, 4.1606, 4.4374, 3.0744, 2.3491, 1.6055, 1.4328, 1.3646, -1.5562, -1.4611, -1.1765]\n",
      "[{'nom': 'La Rochelle', 'centre': {'type': 'Point', 'coordinates': [-1.1765, 46.1621]}, 'code': '17300', '_score': 0.05525189117447229}]\n"
     ]
    }
   ],
   "source": [
    "# Initialiser les listes pour stocker les latitudes et longitudes\n",
    "coordinates_lat = []\n",
    "coordinates_lon = []\n",
    "\n",
    "# Boucler sur chaque ville dans la liste cities_list\n",
    "for city in cities_list:\n",
    "    # Remplacer les espaces par des '+' pour la requ√™te URL\n",
    "    city_query = city.replace(' ', '+')\n",
    "    \n",
    "    # Construire l'URL de la requ√™te\n",
    "    url = f\"https://geo.api.gouv.fr/communes?nom={city_query}&fields=nom,centre,boost=population&limit=1\"\n",
    "    \n",
    "   # Envoyer la requ√™te GET\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    # V√©rifier si la r√©ponse contient des donn√©es\n",
    "    if data:\n",
    "        # Supposer que 'data' est une liste de r√©sultats, prendre le premier\n",
    "        first_result = data[0]\n",
    "        \n",
    "        # Extraire les coordonn√©es\n",
    "        coordinates = first_result['centre']['coordinates']\n",
    "        \n",
    "        # Classer les coordonn√©es\n",
    "        coord_lon = coordinates[0]  # Longitude\n",
    "        coord_lat = coordinates[1]  # Latitude\n",
    "        \n",
    "        # Ajouter les coordonn√©es aux listes\n",
    "        coordinates_lat.append(coord_lat)\n",
    "        coordinates_lon.append(coord_lon)\n",
    "\n",
    "# Afficher les listes de latitudes et longitudes\n",
    "print(\"Latitudes:\", coordinates_lat)\n",
    "print(\"Longitudes:\", coordinates_lon)\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Le Mont-Saint-Michel\n",
       "1                   Saint-Malo\n",
       "2                       Bayeux\n",
       "3                     Le Havre\n",
       "4                        Rouen\n",
       "5                        Paris\n",
       "6                       Amiens\n",
       "7                        Lille\n",
       "8                   Strasbourg\n",
       "9                  Orschwiller\n",
       "10                      Colmar\n",
       "11                   Eguisheim\n",
       "12                    Besancon\n",
       "13                       Dijon\n",
       "14                      Annecy\n",
       "15                    Grenoble\n",
       "16                        Lyon\n",
       "17         La Palud-sur-Verdon\n",
       "18          Bormes-les-Mimosas\n",
       "19                      Cassis\n",
       "20                   Marseille\n",
       "21             Aix-en-Provence\n",
       "22                     Avignon\n",
       "23                        Uzes\n",
       "24                       Nimes\n",
       "25               Aigues-Mortes\n",
       "26    Saintes-Maries-de-la-mer\n",
       "27                   Collioure\n",
       "28                 Carcassonne\n",
       "29                      Ariege\n",
       "30                    Toulouse\n",
       "31                   Montauban\n",
       "32                    Biarritz\n",
       "33                     Bayonne\n",
       "34                 La Rochelle\n",
       "Name: city, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_booking = pd.DataFrame(cities_list, columns=[\"city\"])\n",
    "df_booking[\"city\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 23:17:39 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: scrapybot)\n",
      "2024-06-18 23:17:39 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 23.10.0, Python 3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 24.0.0 (OpenSSL 3.0.13 30 Jan 2024), cryptography 42.0.2, Platform Windows-10-10.0.22631-SP0\n",
      "2024-06-18 23:17:39 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-06-18 23:17:39 [py.warnings] WARNING: c:\\Users\\dsgat\\anaconda3\\Lib\\site-packages\\scrapy\\utils\\request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2024-06-18 23:17:39 [scrapy.extensions.telnet] INFO: Telnet Password: c01af426af830ba6\n",
      "2024-06-18 23:17:40 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-06-18 23:17:40 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2024-06-18 23:17:40 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-06-18 23:17:40 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-06-18 23:17:40 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-06-18 23:17:40 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-06-18 23:17:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-06-18 23:17:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2024-06-18 23:17:41 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2024-06-18 23:17:41 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: booking.json\n",
      "2024-06-18 23:17:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 255,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 204361,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 1.660082,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2024, 6, 18, 21, 17, 41, 905647, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 1258841,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'log_count/INFO': 11,\n",
      " 'log_count/WARNING': 1,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2024, 6, 18, 21, 17, 40, 245565, tzinfo=datetime.timezone.utc)}\n",
      "2024-06-18 23:17:41 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "\n",
    "class BookingSpider(scrapy.Spider):\n",
    "    name = 'booking'\n",
    "    start_urls = ['https://www.booking.com/searchresults.fr.html?dest_id=-1456928;dest_type=city']\n",
    "\n",
    "    def parse(self, response):\n",
    "        hotels = response.css('div.fe_accommodationlist_item')\n",
    "\n",
    "        for hotel in hotels[:20]:\n",
    "            yield {\n",
    "                'name': hotel.css('span.ap-name::text').get(),\n",
    "                'score': hotel.css('div.bui-review-score__badge::text').get(),\n",
    "                'price': hotel.css('div.priceInfo::text').get().strip(),\n",
    "                'url': 'https://www.booking.com' + hotel.css('a.accommodation-link::attr(href)').get()\n",
    "            }\n",
    "\n",
    "        next_page = response.css('a.paging-next::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, callback=self.parse)\n",
    "\n",
    "\n",
    "# Name of the file where the results will be saved\n",
    "filename = \"booking.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will \n",
    "# concatenate the last and new results otherwise)\n",
    "if filename in os.listdir():\n",
    "        os.remove(filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "## USER_AGENT => Simulates a browser on an OS\n",
    "## LOG_LEVEL => Minimal Level of Log \n",
    "## FEEDS => Where the file will be stored \n",
    "## More info on built-in settings => https://docs.scrapy.org/en/latest/topics/settings.html?highlight=settings#settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Chrome/97.0',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        filename : {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(BookingSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (2021251326.py, line 36)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 36\u001b[1;36m\u001b[0m\n\u001b[1;33m    if filename in os.listdir()):\u001b[0m\n\u001b[1;37m                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "class booking_spider(scrapy.Spider):\n",
    "    # Name of your spider\n",
    "    name = \"booking\"\n",
    "\n",
    "    # Url to start your spider from \n",
    "    start_urls = [\n",
    "        'https://www.booking.com/index.fr.html'\n",
    "    ]\n",
    "\n",
    "    # Callback function that will be called when starting your spider\n",
    "    # It will ranking, the title, the url, the total earnings, the rating and the number of voters of the first <div> with class=\"quote\"\n",
    "\n",
    "    def parse(self, response):\n",
    "        elements = response.css(\"div.sc-b189961a-0.hBZnfJ.cli-children\")\n",
    "        for element in elements:\n",
    "            yield {\n",
    "                \"ranking\" : element.css('h3.ipc-title__text::text').get().split(\".\")[0]\n",
    "                ,\n",
    "                \"title\" : element.css(\"h3.ipc-title__text::text\").get().split(\".\")[1].strip()\n",
    "                ,\n",
    "                \"url\" : element.css('a.ipc-title-link-wrapper').attrib[\"href\"]\n",
    "                ,\n",
    "                \"total_earning\" : element.css('span.sc-8f57e62c-2.elpuzG::text').get()\n",
    "                ,\n",
    "                \"rating\" : element.css('span.ipc-rating-star.ipc-rating-star--base.ipc-rating-star--imdb.ratingGroup--imdb-rating::text').get()\n",
    "                ,\n",
    "                \"voters\" : element.css('span.ipc-rating-star--voteCount::text').getall()[1]\n",
    "            }\n",
    "        \n",
    "    \n",
    "# Name of the file where the results will be saved\n",
    "filename = \"imdb1.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will \n",
    "# concatenate the last and new results otherwise)\n",
    "if filename in os.listdir()):\n",
    "        os.remove(filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "## USER_AGENT => Simulates a browser on an OS\n",
    "## LOG_LEVEL => Minimal Level of Log \n",
    "## FEEDS => Where the file will be stored \n",
    "## More info on built-in settings => https://docs.scrapy.org/en/latest/topics/settings.html?highlight=settings#settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Chrome/97.0',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        filename : {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(imdb_spider)\n",
    "process.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
